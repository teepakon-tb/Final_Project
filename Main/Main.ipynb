{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data From Elasticsearch and Safe to data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def getdata():\n",
    "    es = Elasticsearch(['http://10.251.151.76:9200'])\n",
    "    \n",
    "    # Specify the time range for data selection\n",
    "    start_time = datetime.now() - timedelta(minutes=5)\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Dynamically construct the index name based on the current date\n",
    "    index_date = end_time.strftime(\"%Y.%m.%d\")\n",
    "    index_name = f\"logstash-test-{index_date}\"\n",
    "    \n",
    "    scroll_size = 10000\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"range\": {\"@timestamp\": {\"gte\": start_time, \"lte\": end_time}}},  # Filter by timestamp\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": scroll_size,\n",
    "    }\n",
    "\n",
    "    response = es.search(index=index_name, body=search_body, scroll='100m')\n",
    "    scroll_id = response['_scroll_id']\n",
    "    results = []\n",
    "\n",
    "    while True:\n",
    "        hits = response['hits']['hits']\n",
    "        if not hits:\n",
    "            break\n",
    "        results.extend([hit['_source'] for hit in hits])\n",
    "        response = es.scroll(scroll_id=scroll_id, scroll='100m')\n",
    "    \n",
    "    # Convert the results to JSON format\n",
    "    json_results = json.dumps(results, indent=2)\n",
    "    \n",
    "    # Save the results to a file named 'data.json'\n",
    "    with open('data.json', 'w') as f:\n",
    "        f.write(json_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data from a JSON file and perform feature engineering to achieve the desired features, you can follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "segments = {}\n",
    "# ฟังก์ชันสำหรับ segment packets และแสดง feature ของแต่ละ segment\n",
    "def segment_packets(packets):\n",
    "    global segments  # เก็บ segment แต่ละอันพร้อม feature\n",
    "    for packet in packets:\n",
    "        src_ip = packet.get('srcip', None)\n",
    "        dst_ip = packet.get('dstip', None)\n",
    "        src_port = packet.get('srcport', None)\n",
    "        dst_port = packet.get('dstport', None)\n",
    "        sentbyte = int(packet.get('sentbyte', 0))  # อ่านค่า sbytes และแปลงเป็นจำนวนเต็ม\n",
    "        rcvdbyte = int(packet.get('rcvdbyte', 0))  # อ่านค่า dbytes และแปลงเป็นจำนวนเต็ม\n",
    "        sentpkt = int(packet.get('sentpkt', 0))  # อ่านค่า spkts และแปลงเป็นจำนวนเต็ม\n",
    "        rcvdpkt = int(packet.get('rcvdpkt', 0))  # อ่านค่า dpkts และแปลงเป็นจำนวนเต็ม\n",
    "        service = packet.get('service', None)\n",
    "        timestamp = packet.get('timestamp', None)  # เพิ่มการดึงค่า timestamp\n",
    "        duration = packet.get('duration', None)\n",
    "\n",
    "        segment_key = (src_ip, src_port, dst_ip, dst_port)  # กำหนด segment_key ใหม่\n",
    "\n",
    "\n",
    "        # คำนวณค่า dinpkt และ sinpkt สำหรับแต่ละ packet\n",
    "        if sentpkt > 1:\n",
    "            duration = int(duration)\n",
    "            dinpkt = duration / (sentpkt - 1)\n",
    "            packet['dinpkt'] = dinpkt\n",
    "        if rcvdpkt > 1:\n",
    "            duration = int(duration)\n",
    "            sinpkt = duration / (rcvdpkt - 1)\n",
    "            packet['sinpkt'] = sinpkt\n",
    "\n",
    "        if segment_key not in segments:\n",
    "            segments[segment_key] = {'packets': [], 'ct_srv_src': 0, 'is_sm_ips_ports': 0, 'is_ftp_login': 0, 'ct_srv_dst': 0, 'ct_dst_ltm': 0, 'ct_src_ltm': 0, 'ct_src_dport_ltm': 0, 'ct_dst_sport_ltm': 0 ,'ct_dst_src_ltm': 0,'last_timestamp': None, 'sbytes': 0, 'dbytes': 0, 'spkts': 0, 'dpkts': 0,'response_body_len': 0 }  # เพิ่ม last_timestamp, sbytes, dbytes, spkts, และ dpkts ใน segment\n",
    "        segments[segment_key]['packets'].append(packet)  # เพิ่มข้อมูลเรียบร้อย\n",
    "        \n",
    "        segments[segment_key]['sbytes'] += sentbyte  # เพิ่มขนาดของข้อมูลที่ถูกส่งออก\n",
    "        segments[segment_key]['dbytes'] += rcvdbyte  # เพิ่มขนาดของข้อมูลที่ถูกรับเข้า\n",
    "        segments[segment_key]['spkts'] += sentpkt  # เพิ่มจำนวนของแพ็กเก็ตที่ถูกส่งออก\n",
    "        segments[segment_key]['dpkts'] += rcvdpkt  # เพิ่มจำนวนของแพ็กเก็ตที่ถูกรับเข้า\n",
    "\n",
    "        segments[segment_key]['packets'].append(packet)  # เพิ่มข้อมูลเรียบร้อย\n",
    "        if timestamp and (not segments[segment_key]['last_timestamp'] or timestamp > segments[segment_key]['last_timestamp']):\n",
    "            segments[segment_key]['last_timestamp'] = timestamp  # กำหนด last_timestamp เป็น timestamp ล่าสุด\n",
    "\n",
    "    # คำนวณค่า ct_srv_src, is_sm_ips_ports, is_ftp_login, ct_srv_dst, ct_dst_ltm และ ct_src_ltm สำหรับแต่ละ segment\n",
    "    for segment, data in segments.items():\n",
    "        # คำนวณ ct_srv_src\n",
    "        service_count_src = {}\n",
    "        for packet in data['packets']:\n",
    "            src_ip = packet.get('srcip', None)\n",
    "            service_key = packet.get('service', None)\n",
    "            if service_key and src_ip:\n",
    "                connection_key = (src_ip, service_key)\n",
    "                if connection_key in service_count_src:\n",
    "                    service_count_src[connection_key] += 1\n",
    "                else:\n",
    "                    service_count_src[connection_key] = 1\n",
    "        max_count_src = max(service_count_src.values()) if service_count_src else 0\n",
    "        data['ct_srv_src'] = max_count_src if max_count_src <= 100 else 100  # ระบุค่าเท่ากับจำนวนการเชื่อมต่อสูงสุด หรือ 100 ตามเงื่อนไข\n",
    "\n",
    "        # คำนวณ is_sm_ips_ports\n",
    "        is_sm_ips_ports = 1 if segment[0] == segment[2] and segment[1] == segment[3] else 0\n",
    "        data['is_sm_ips_ports'] = is_sm_ips_ports\n",
    "        \n",
    "        # คำนวณ is_ftp_login\n",
    "        for packet in data['packets']:\n",
    "            if 'ftp' in packet.get('service', '').lower():\n",
    "                if 'user' in packet and 'pass' in packet:\n",
    "                    data['is_ftp_login'] = 1\n",
    "                    break\n",
    "\n",
    "        # คำนวณ ct_srv_dst\n",
    "        service_count_dst = {}\n",
    "        for packet in data['packets']:\n",
    "            dst_ip = packet.get('dstip', None)\n",
    "            service_key = packet.get('service', None)\n",
    "            if service_key and dst_ip:\n",
    "                connection_key = (dst_ip, service_key)\n",
    "                if connection_key in service_count_dst:\n",
    "                    service_count_dst[connection_key] += 1\n",
    "                else:\n",
    "                    service_count_dst[connection_key] = 1\n",
    "        max_count_dst = max(service_count_dst.values()) if service_count_dst else 0\n",
    "        data['ct_srv_dst'] = max_count_dst\n",
    "\n",
    "        # คำนวณ ct_dst_ltm\n",
    "        dst_count = {}\n",
    "        for packet in data['packets']:\n",
    "            dst_ip = packet.get('dstip', None)\n",
    "            if dst_ip:\n",
    "                if dst_ip in dst_count:\n",
    "                    dst_count[dst_ip] += 1\n",
    "                else:\n",
    "                    dst_count[dst_ip] = 1\n",
    "        data['ct_dst_ltm'] = len(dst_count)\n",
    "\n",
    "        # คำนวณ ct_src_ltm\n",
    "        src_count = {}\n",
    "        for packet in data['packets']:\n",
    "            src_ip = packet.get('srcip', None)\n",
    "            if src_ip:\n",
    "                if src_ip in src_count:\n",
    "                    src_count[src_ip] += 1\n",
    "                else:\n",
    "                    src_count[src_ip] = 1\n",
    "        data['ct_src_ltm'] = len(src_count)\n",
    "\n",
    "        # คำนวณ ct_src_dport_ltm\n",
    "        src_dport_count = {}\n",
    "        for packet in data['packets']:\n",
    "            src_ip = packet.get('srcip', None)\n",
    "            dst_port = packet.get('dstport', None)\n",
    "            if src_ip and dst_port:\n",
    "                connection_key = (src_ip, dst_port)\n",
    "                if connection_key in src_dport_count:\n",
    "                    src_dport_count[connection_key] += 1\n",
    "                else:\n",
    "                    src_dport_count[connection_key] = 1\n",
    "\n",
    "        ct_src_dport_ltm = sum(1 for count in src_dport_count.values() if count <= 100)\n",
    "        data['ct_src_dport_ltm'] = ct_src_dport_ltm\n",
    "\n",
    "        # คำนวณ ct_dst_sport_ltm\n",
    "        dst_sport_count = {}\n",
    "        for packet in data['packets']:\n",
    "            dst_ip = packet.get('dstip', None)\n",
    "            src_port = packet.get('srcport', None)\n",
    "            if dst_ip and src_port:\n",
    "                connection_key = (dst_ip, src_port)\n",
    "                if connection_key in dst_sport_count:\n",
    "                    dst_sport_count[connection_key] += 1\n",
    "                else:\n",
    "                    dst_sport_count[connection_key] = 1\n",
    "\n",
    "        ct_dst_sport_ltm = sum(1 for count in dst_sport_count.values() if count <= 100)\n",
    "        data['ct_dst_sport_ltm'] = ct_dst_sport_ltm\n",
    "\n",
    "        # คำนวณ ct_dst_src_ltm\n",
    "        dst_src_count = {}\n",
    "        for packet in data['packets']:\n",
    "            src_ip = packet.get('srcip', None)\n",
    "            dst_ip = packet.get('dstip', None)\n",
    "            if src_ip and dst_ip:\n",
    "                connection_key = (src_ip, dst_ip)\n",
    "                if connection_key in dst_src_count:\n",
    "                    dst_src_count[connection_key] += 1\n",
    "                else:\n",
    "                    dst_src_count[connection_key] = 1\n",
    "\n",
    "        ct_dst_src_ltm = sum(1 for count in dst_src_count.values() if count <= 100)\n",
    "        data['ct_dst_src_ltm'] = ct_dst_src_ltm\n",
    "\n",
    "        # คำนวณฟีเจอร์ response_body_len\n",
    "        response_body_len = int(packet.get('rcvdbyte', 0)) - int(packet.get('sentbyte', 0))\n",
    "        data['response_body_len'] = response_body_len\n",
    "\n",
    "    # แก้ไขส่วนการ segment_packets()\n",
    "         \n",
    "    for segment, data in segments.items():\n",
    "        total_sbytes = 0  # เพิ่มตัวแปรสำหรับเก็บค่ารวมของ sbytes\n",
    "        total_dbytes = 0  # เพิ่มตัวแปรสำหรับเก็บค่ารวมของ dbytes\n",
    "        total_spkts = 0   # เพิ่มตัวแปรสำหรับเก็บค่ารวมของ spkts\n",
    "        total_dpkts = 0   # เพิ่มตัวแปรสำหรับเก็บค่ารวมของ dpkts\n",
    "        \n",
    "        for packet in data['packets']:\n",
    "            total_sbytes += int(packet.get('sentbyte', 0))\n",
    "            total_dbytes += int(packet.get('rcvdbyte', 0))\n",
    "            total_spkts += int(packet.get('sentpkt', 0))\n",
    "            total_dpkts += int(packet.get('rcvdpkt', 0))\n",
    "        \n",
    "        data['sbytes'] = total_sbytes\n",
    "        data['dbytes'] = total_dbytes\n",
    "        data['spkts'] = total_spkts\n",
    "        data['dpkts'] = total_dpkts\n",
    "     # เพิ่ม total_packets ด้วยจำนวนของ spkts และ dpkts\n",
    "        total_packets = data['spkts'] + data['dpkts']\n",
    "        data['total_packets'] = total_packets\n",
    "    \n",
    "    return segments\n",
    "\n",
    "# ฟังก์ชันหลักสำหรับอ่านข้อมูลจากไฟล์ JSON และสร้าง packets\n",
    "def main(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    packets = []  # เก็บ packets ที่อ่านจากไฟล์ JSON\n",
    "    for entry in data:\n",
    "        packets.append(entry if entry else None)  #\n",
    "\n",
    "    segments = segment_packets(packets)\n",
    "\n",
    "    # นับและแสดงจำนวน segment ทั้งหมด\n",
    "    print(\"Total segments:\", len(segments))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"data.json\"  # ชื่อไฟล์ JSON ที่ต้องการใช้\n",
    "    main(filename)\n",
    "\n",
    "    import csv\n",
    "\n",
    "def save_segments_to_csv(segments, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['src_ip', 'src_port', 'dst_ip', 'dst_port', 'dinpkt', 'sinpkt', 'sbytes', 'dbytes', 'spkts', 'dpkts', 'ct_srv_src', 'is_sm_ips_ports', 'is_ftp_login', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'response_body_len']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for segment, data in segments.items():\n",
    "            src_ip, src_port, dst_ip, dst_port = segment\n",
    "            writer.writerow({\n",
    "                'src_ip': src_ip,\n",
    "                'src_port': src_port,\n",
    "                'dst_ip': dst_ip,\n",
    "                'dst_port': dst_port,\n",
    "                'dinpkt': data['packets'][0].get('dinpkt', '0'),\n",
    "                'sinpkt': data['packets'][0].get('sinpkt', '0'),\n",
    "                'sbytes': data['sbytes'],\n",
    "                'dbytes': data['dbytes'],\n",
    "                'spkts': data['spkts'],\n",
    "                'dpkts': data['dpkts'],\n",
    "                'ct_srv_src': data['ct_srv_src'],\n",
    "                'is_sm_ips_ports': data['is_sm_ips_ports'],\n",
    "                'is_ftp_login': data['is_ftp_login'],\n",
    "                'ct_srv_dst': data['ct_srv_dst'],\n",
    "                'ct_dst_ltm': data['ct_dst_ltm'],\n",
    "                'ct_src_ltm': data['ct_src_ltm'],\n",
    "                'ct_src_dport_ltm': data['ct_src_dport_ltm'],\n",
    "                'ct_dst_sport_ltm': data['ct_dst_sport_ltm'],\n",
    "                'ct_dst_src_ltm': data['ct_dst_src_ltm'],\n",
    "                'response_body_len': data['response_body_len']\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def ml_process(data_path='segments.csv', model_path='rf_classifier+FS+DS2.pkl', preprocessor_path='preprocessor+FS+DS2.pkl', threshold=0.75, output_path='high_attack_prob_records.csv'):\n",
    "    # Load model and preprocessor\n",
    "    rf_model = joblib.load(model_path)\n",
    "    preprocessor = joblib.load(preprocessor_path)\n",
    "\n",
    "    # Load new data\n",
    "    new_data = pd.read_csv(data_path)\n",
    "\n",
    "    # Define features\n",
    "    new_features = ['sbytes', 'dbytes', 'spkts', 'dpkts',\n",
    "                    'response_body_len', 'sinpkt', 'dinpkt', 'is_sm_ips_ports', \n",
    "                    'is_ftp_login', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', \n",
    "                    'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm']\n",
    "\n",
    "    # Preprocess new data\n",
    "    X_test_new = new_data[new_features]\n",
    "    X_new = preprocessor.transform(X_test_new)\n",
    "\n",
    "    # Predict probabilities\n",
    "    predictions_proba = rf_model.predict_proba(X_new)\n",
    "\n",
    "    # Create DataFrame from probabilities\n",
    "    probabilities_df = pd.DataFrame(predictions_proba, columns=['Normal', 'Attack'])\n",
    "\n",
    "    # Filter records with attack probability greater than threshold\n",
    "    high_attack_prob_records = probabilities_df[probabilities_df['Attack'] > threshold]\n",
    "\n",
    "    # Extract additional features from the original data\n",
    "    segments = {}\n",
    "    for index, packet in new_data.iterrows():\n",
    "        src_ip = packet.get('src_ip', None)\n",
    "        src_port = packet.get('src_port', None)\n",
    "        dst_ip = packet.get('dst_ip', None)\n",
    "        dst_port = packet.get('dst_port', None)\n",
    "        segment_key = (src_ip, src_port, dst_ip, dst_port)\n",
    "        if segment_key not in segments:\n",
    "            segments[segment_key] = []\n",
    "        segments[segment_key].append(packet)\n",
    "\n",
    "    # Add additional features to the DataFrame\n",
    "    additional_features = {\n",
    "        'src_ip': [],\n",
    "        'src_port': [],\n",
    "        'dst_ip': [],\n",
    "        'dst_port': [],\n",
    "        'dinpkt': [],\n",
    "        'sinpkt': [],\n",
    "        'sbytes': [],\n",
    "        'dbytes': [],\n",
    "        'spkts': [],\n",
    "        'dpkts': [],\n",
    "        'ct_srv_src': [],\n",
    "        'is_sm_ips_ports': [],\n",
    "        'is_ftp_login': [],\n",
    "        'ct_srv_dst': [],\n",
    "        'ct_dst_ltm': [],\n",
    "        'ct_src_ltm': [],\n",
    "        'ct_src_dport_ltm': [],\n",
    "        'ct_dst_sport_ltm': [],\n",
    "        'ct_dst_src_ltm': [],\n",
    "        'response_body_len': []\n",
    "    }\n",
    "\n",
    "    for segment, data in segments.items():\n",
    "        additional_features['src_ip'].append(segment[0])\n",
    "        additional_features['src_port'].append(segment[1])\n",
    "        additional_features['dst_ip'].append(segment[2])\n",
    "        additional_features['dst_port'].append(segment[3])\n",
    "        additional_features['dinpkt'].append(data[0].get('dinpkt', '0'))\n",
    "        additional_features['sinpkt'].append(data[0].get('sinpkt', '0'))\n",
    "        additional_features['sbytes'].append(sum(int(packet.get('sbytes', 0)) for packet in data))\n",
    "        additional_features['dbytes'].append(sum(int(packet.get('dbytes', 0)) for packet in data))\n",
    "        additional_features['spkts'].append(sum(int(packet.get('spkts', 0)) for packet in data))\n",
    "        additional_features['dpkts'].append(sum(int(packet.get('dpkts', 0)) for packet in data))\n",
    "        additional_features['ct_srv_src'].append(data[0].get('ct_srv_src', 0))\n",
    "        additional_features['is_sm_ips_ports'].append(data[0].get('is_sm_ips_ports', 0))\n",
    "        additional_features['is_ftp_login'].append(data[0].get('is_ftp_login', 0))\n",
    "        additional_features['ct_srv_dst'].append(data[0].get('ct_srv_dst', 0))\n",
    "        additional_features['ct_dst_ltm'].append(data[0].get('ct_dst_ltm', 0))\n",
    "        additional_features['ct_src_ltm'].append(data[0].get('ct_src_ltm', 0))\n",
    "        additional_features['ct_src_dport_ltm'].append(data[0].get('ct_src_dport_ltm', 0))\n",
    "        additional_features['ct_dst_sport_ltm'].append(data[0].get('ct_dst_sport_ltm', 0))\n",
    "        additional_features['ct_dst_src_ltm'].append(data[0].get('ct_dst_src_ltm', 0))\n",
    "        additional_features['response_body_len'].append(sum(int(packet.get('response_body_len', 0)) for packet in data))\n",
    "\n",
    "    # Combine features with high attack probability records\n",
    "    for feature, values in additional_features.items():\n",
    "        high_attack_prob_records.loc[:, feature] = pd.Series(values)\n",
    "\n",
    "    # Save records to CSV\n",
    "    high_attack_prob_records.to_csv(output_path, index=False)\n",
    "\n",
    "    return high_attack_prob_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send high_attack_prob_records.csv to Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import os.path\n",
    "\n",
    "def send_email_with_attachment(email_sender, email_password, email_receiver, subject, body, filename):\n",
    "    # ตรวจสอบว่าไฟล์ CSV มีข้อมูลหรือไม่\n",
    "    if os.path.isfile(filename) and os.path.getsize(filename) > 0:\n",
    "        # เปิดไฟล์\n",
    "        attachment = open(filename, 'rb')\n",
    "\n",
    "        # สร้าง MIMEBase object\n",
    "        part = MIMEBase('application', 'octet-stream')\n",
    "        part.set_payload((attachment).read())\n",
    "        encoders.encode_base64(part)\n",
    "        part.add_header('Content-Disposition', \"attachment; filename= %s\" % filename)\n",
    "\n",
    "        # สร้าง MIMEMultipart object\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = email_sender\n",
    "        msg['To'] = email_receiver\n",
    "        msg['Subject'] = subject\n",
    "\n",
    "        # เพิ่มเนื้อหาข้อความ\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        # เพิ่มไฟล์แนบลงใน MIMEMultipart object\n",
    "        msg.attach(part)\n",
    "\n",
    "        # เชื่อมต่อกับ SMTP server ของ Gmail\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        server.starttls()\n",
    "\n",
    "        # เข้าสู่ระบบ\n",
    "        server.login(email_sender, email_password)\n",
    "\n",
    "        # ส่งอีเมล\n",
    "        server.send_message(msg)\n",
    "\n",
    "        # ปิดการเชื่อมต่อ\n",
    "        server.quit()\n",
    "\n",
    "        print(\"Email ถูกส่งเรียบร้อยแล้ว\")\n",
    "    else:\n",
    "        print(\"ไม่มีข้อมูลในไฟล์ CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def orchestrate_execution():\n",
    "    while True:\n",
    "        # Run Elasticsearch data retrieval\n",
    "        getdata()  \n",
    "        \n",
    "        # Process the retrieved data\n",
    "        main(\"data.json\")  \n",
    "        \n",
    "        # Save processed data to CSV\n",
    "        save_segments_to_csv(segments, 'segments.csv')  \n",
    "\n",
    "        # Execute ML process\n",
    "        ml_process(\"segments.csv\")\n",
    "\n",
    "        # Send email with attachment\n",
    "        email_sender = 'mail'\n",
    "        email_password = 'password'\n",
    "        email_receiver = 'mail'\n",
    "        subject = 'High Attack Probability Records'\n",
    "        body = 'Please find attached the high attack probability records.'\n",
    "        filename = 'high_attack_prob_records.csv'\n",
    "        \n",
    "        send_email_with_attachment(email_sender, email_password, email_receiver, subject, body, filename)\n",
    "\n",
    "        time.sleep(300)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    orchestrate_execution()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
